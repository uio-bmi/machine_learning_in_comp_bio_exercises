{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 2: transcription factor binding prediction (with hyperparameter optimization)\n",
    "\n",
    "In this exercise, we will train multiple models allowing us to get more robust estimate of the performance. We will also compare the models in a cross-validation setting to find the best one.\n",
    "\n",
    "Data is the same as in the previous exercise: it contains a list of DNA sequences that are annotated with 1 if the sequence binds the transcription factor or 0 if not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run if on google colab to get all the files from github\n",
    "\n",
    "!git clone https://github.com/uio-bmi/machine_learning_in_comp_bio_exercises.git\n",
    "!mv ./machine_learning_in_comp_bio_exercises/{.,}* ./\n",
    "!rm -r ./machine_learning_in_comp_bio_exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scripts.util import load_data_exercise_2, load_test_dataset_ex2, encode_kmer, print_dataset_info, train_logistic_regression, assess_model, make_folder\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_exercise_2()\n",
    "print_dataset_info(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Setting up cross-validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get indices of examples in the dataset\n",
    "\n",
    "indices = list(range(dataset.shape[0]))\n",
    "print(indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a folder to store temporary results\n",
    "\n",
    "result_path = \"./exercise_2_output/\"\n",
    "make_folder(result_path)\n",
    "\n",
    "# initialize a list to store performances\n",
    "\n",
    "performances = []\n",
    "\n",
    "# k-fold cross-validation setup\n",
    "\n",
    "k_fold = KFold(n_splits=2)\n",
    "current_split = 1 # to know what is the currect split\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(indices):\n",
    "    \n",
    "    # split the data\n",
    "    \n",
    "    train_dataset = dataset.iloc[train_indices, :]\n",
    "    test_dataset = dataset.iloc[test_indices, :]\n",
    "    \n",
    "    print_dataset_info(train_dataset, test_dataset)\n",
    "    \n",
    "    # TODO: how to encode the data?\n",
    "    \n",
    "    # encoded_train, train_labels, feature_names = ?\n",
    "    # encoded_test, test_labels, _ = ?\n",
    "    \n",
    "    # TODO: how to train and asses a model?\n",
    "    \n",
    "    # logistic_regression = ?\n",
    "    \n",
    "    current_split += 1 # go to next split\n",
    "    \n",
    "print(performances)\n",
    "\n",
    "# what is the expected performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Comparing different hyperparameters\n",
    "\n",
    "What if we were interested in different subsequence length (different k)? How would we compare them? Modify the skeleton below to obtain performance estimates for two different values of k and compare them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a folder to store temporary results\n",
    "\n",
    "result_path = \"./exercise_2_output_hp/\"\n",
    "make_folder(result_path)\n",
    "\n",
    "# TODO: provide a list of k values to test for\n",
    "k_values = []\n",
    "\n",
    "# performance measures could be stored in a format: \n",
    "# {k_5: [accuracy_split1, accuracy_split2]}, k_6: [accuracy_split1, accuracy_split2] }\n",
    "performances = {f\"k_{k}\": [] for k in k_values}\n",
    "\n",
    "# logistic regression trained models will be stored here:\n",
    "log_reg_models = []\n",
    "\n",
    "# k-fold cross validation setup\n",
    "\n",
    "k_fold = KFold(n_splits=2)\n",
    "\n",
    "for train_indices, test_indices in k_fold.split(indices):\n",
    "    \n",
    "    # split the data\n",
    "    \n",
    "    train_dataset = dataset.iloc[train_indices, :]\n",
    "    test_dataset = dataset.iloc[test_indices, :]\n",
    "    \n",
    "    print_dataset_info(train_dataset, test_dataset)\n",
    "    \n",
    "    for k in k_values:\n",
    "    \n",
    "        # TODO: how to encode the data?\n",
    "\n",
    "        # encoded_train, train_labels, feature_names = ?\n",
    "        # encoded_test, test_labels, _ = ?\n",
    "\n",
    "        # TODO: how to train and test a model?\n",
    "\n",
    "        # logistic_regression = ?\n",
    "        \n",
    "        # TODO: how to check the accuracy? (hint: sklearn has accuracy_score function -- https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)\n",
    "        \n",
    "        acc = 0\n",
    "        \n",
    "        performances[f\"k_{k}\"].append(acc)\n",
    "    \n",
    "print(performances)\n",
    "\n",
    "# which k is better? (hint: which k has higher accuracy on average?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: what if we wanted to compare different k values, but also compare different ML methods?\n",
    "\n",
    "Try to set up CV with 2 different k values for k-mer frequency encoding and with 2 different ML methods, e.g., logistic regression and random forest with default hyperparameters. Alternatively, try different hyperparameter values for one model, e.g., try varying regularization strength (C) parameter of the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: How well does the selected best model perform on the new test dataset?\n",
    "\n",
    "Assess the performance of the chosen best model on a new dataset and compare it with the performances obtained during cross validation. Is there a difference? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = load_test_dataset_ex2()\n",
    "print_dataset_info(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: assess the performance of the model\n",
    "\n",
    "# what are the steps here?\n",
    "\n",
    "performance = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}